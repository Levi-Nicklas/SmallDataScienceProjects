---
title: "Linear Discriminant Analysis"
output: html_notebook
---



```{r, message = F, include=FALSE}
library(dslabs)
library(tidyverse)
library(caret)
```

# Project

This project is a demonstration of linear discriminant analysis in `R` using popular tools from the `tidyverse`. In this project I examine a dataset that contains information on the acids found in olive oils.

![](https://cdn.drweil.com/wp-content/uploads/2016/12/diet-nutrition_nutrition_olive-oil-fraud_2382x2063_000079492729.jpg)

# The Data
Let's look at this data.
```{r}
head(dslabs::olive)
```




So I want to classify the region, area, or a combination of the two based on the acids in the olive oils. Let's see how many classes we are going to try to predict.

```{r}
dslabs::olive %>% 
  group_by(region, area) %>% 
  count()
```

So there are 3 regions, and 9 areas. Since each area only belongs to a single region, we can either predict 3 `region` classes or 9 `area` classes. 

I'll begin by predicting the region classes.

# Region Classification

## Pre-Processing
I think we have some pre-processing to do before we jump into modeling. 

```{r}
dslabs::olive %>% 
  group_by(region) %>% 
  count() %>% 
  ggplot(aes(region, n, fill = region)) +
  geom_col() +
  scale_fill_brewer(palette = "Accent") +
  labs(x = "Region", y = "Count", fill = "",
       title = "dslabs::olive Class Balance") +
  theme_classic() 
```

First, we have some class imbalance that may be good to address. 

Second, while we arent dealing with extremely high dimensionality, we have enough independant variables to make visualizaion difficult. For that reason, before plotting we will perform a prinicipal component analysis transform and see if we can operate in two dimensions moving forward. 

Third, we need to split classes here. I'm thinking we will split after dealing with the class imbalance, and save some data for validation. We will do k-folding to get accurate estimates of what the parameters are.


### Class Imbalance
Let's begin with the class imbalance. Since Sardina has 98 entries, I will sample 80 of those, and only sample 98 from each of the other two regions as well to balance the dataset. This will leave us with 240 observations that we will build our classifier on. We can use the extra observations as validation data to check how robust our model is.

```{r}
set.seed(23)

NI_olives <- dslabs::olive %>% 
  filter(region == "Northern Italy") %>% 
  sample_n(80, replace = F)

SI_olives <- dslabs::olive %>% 
  filter(region == "Southern Italy") %>% 
  sample_n(80, replace = F)

SD_olives <- dslabs::olive %>% 
  filter(region == "Sardinia") %>% 
  sample_n(80, replace = F)

balanced_data <- rbind(NI_olives, SI_olives, SD_olives)
rm(NI_olives, SI_olives, SD_olives)

summary(balanced_data)
```

We ended up with some imbalace in the `area` sub-class, but that is not our concern right now, so we will proceed.

### Dimensionality Reduction

Since we have 8 different acids that act as predictors in our dataset, I am going to see if I can reduce the dimensionality with principal component analysis.

```{r}
bal_X <- balanced_data[,c(-1,-2)]
bal_Y <- balanced_data[,c(1,2)]

library(corrplot)
cor(bal_X) %>% 
  corrplot()
```

Our data looks like there are some strong correlations between different variables; the principal component analysis should work great on this data.

But first, since our data have differing numeric ranges, we should standardize the data prior to performing PCA.

```{r}
bal_olive_pca <- prcomp(bal_X, center = TRUE, scale. = TRUE)
summary(bal_olive_pca)
```

Somewhat disappointingly, the PCA only captures 64% of the variance in the first two components. We can still use this for plotting, but at this point we dont gain much simplicity by using 5 predictors as opposed to 8. SO for modeling purposes, I am *not* going to include a PCA preprocessing step. 

But I am going to hold onto the PCA rotation, and we can use the first two components for plotting our data points and look at the classification of our points. First, let's look at the `balanced_data` that we made.

```{r}
pc12_olives_bal <- data.frame(as.factor(balanced_data$region),
                         bal_olive_pca$x[,1],
                         bal_olive_pca$x[,2]) 
colnames(pc12_olives_bal) <- c("region", "PC1", "PC2")
pc12_olives_bal <- as.data.frame(pc12_olives_bal)


p1 <- pc12_olives_bal %>% 
  group_by(region) %>% 
  ggplot(aes(PC1, PC2, color = as.factor(region))) +
  geom_point() +
  geom_density_2d()+
  scale_color_brewer(palette = "Accent") +
  labs(x = "PC1", y = "PC2", color = "",
       title = "dslabs::olives classes in PC1 & PC2") +
  theme_classic()+
  theme(legend.position = "bottom")

p1

```
Great to not throw that away! This looks like we should be able seperate these classes very easily.

I am going to try classifying with these first two components, as this will allow for great visualizations.

### Linear Discriminant Analysis

The idea behind linear discriminant analysis is that all of our points are projected on a line, and then the best seperating values are located. This line that we project onto is not necessarily an axis line, but let's see what the distribution of points on the axes looks like.


```{r}
library(ggExtra)

ggExtra::ggMarginal(p1, pc12_olives_bal, x = PC1, y = PC2, groupFill = TRUE)
```

So, if I told you we could *only* use `PC1` or `PC2` for our LDA, which would be best? 

`PC1` would be best. The distributions have the best seperation on this axis. `PC2` we see Northern Italy with a weird bimodal distribution, and Sardina sharing almost the same distribution as Southern Italy.










